{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c952c7980a41aa9dd4b02a980396d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1fc513661a439a9b3ec6557504e2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:  10%|#         | 503M/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11320\\.conda\\envs\\llama3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\11320\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db67b267f08b430e93d3bd6f474bfd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf44030bd2f641f5bea1d4deb7262c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cad949954544c5282b0d54a58bc6ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1d013fd8ec4f548a039b3ea21e03ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'please show how to check all the downloaded models from hugging face cli and all the downloaded cache model\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=2,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    truncation = True,\n",
    "    max_length=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'please show how to check all the downloaded models from hugging face cli and all the downloaded cache model\\nI have a few questions about the hugging face cli.\\n  1. How to check all the downloaded models from the hugging face cli?\\n  2. How to check all the downloaded cache model?\\n  3. How to check all the downloaded tokenizer from the hugging face cli?\\nI am using the hugging face cli to download all the models from the hugging face hub. But I am not sure how to check all the downloaded models and the cache models.\\nI would like to know how to check all the downloaded models and the cache models.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like to know how to check all the downloaded tokenizer.\\nI would like'}\n",
      "{'generated_text': 'please show how to check all the downloaded models from hugging face cli and all the downloaded cache model\\n```\\nimport os\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torchvision.models as models\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data.sampler import SubsetRandomSampler\\nfrom torchvision.datasets import ImageFolder\\nfrom torchvision import transforms\\nfrom torchvision.transforms import ToTensor\\nfrom torchvision.transforms import Normalize\\nfrom torchvision.utils import make_grid\\nfrom torchvision.utils import save_image\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport PIL\\nfrom PIL import Image\\nfrom PIL import ImageDraw\\nfrom PIL import ImageFont\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport matplotlib.patheffects as pe\\nimport random\\nimport cv2\\nimport numpy as np\\nimport os\\nimport sys\\nfrom pathlib import Path\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport matplotlib.patheffects as pe\\nimport random\\nimport cv2\\nimport numpy as np\\nimport os\\nimport sys\\nfrom pathlib import Path\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport matplotlib.patheffects as pe\\nimport random\\nimport cv2\\nimport numpy as np\\nimport os\\nimport sys\\nfrom pathlib import Path\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport matplotlib.patheffects as pe\\nimport random\\nimport cv2\\nimport numpy as np\\nimport os\\nimport sys\\nfrom pathlib import Path\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport matplotlib.patheffects as pe\\nimport random\\nimport cv2\\nimport numpy as np\\nimport os\\nimport sys\\nfrom pathlib import Path\\nimport matplotlib.pyplot as plt'}\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
